{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainScanAI - Analyse Exploratoire\n",
    "\n",
    "## Notebook 1: Analyse exploratoire des données IRM et clustering pour labellisation faible\n",
    "\n",
    "### Objectifs\n",
    "1. Charger et explorer les données IRM\n",
    "2. Extraire des features avec ResNet pré-entraîné\n",
    "3. Réduire la dimensionnalité avec PCA/t-SNE\n",
    "4. Appliquer des algorithmes de clustering (K-Means, DBSCAN)\n",
    "5. Générer des labels faibles pour l'apprentissage semi-supervisé\n",
    "\n",
    "### Prérequis\n",
    "- Python 3.11+\n",
    "- PyTorch/Torchvision\n",
    "- Scikit-learn\n",
    "- Matplotlib/Seaborn\n",
    "\n",
    "### Structure du notebook\n",
    "1. Configuration de l'environnement\n",
    "2. Chargement des données\n",
    "3. Préprocessing des images\n",
    "4. Extraction de features\n",
    "5. Analyse de dimensionnalité\n",
    "6. Clustering\n",
    "7. Visualisation des résultats\n",
    "8. Génération de labels faibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML et Deep Learning\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualisation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path('../data/raw')\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "MODELS_DIR = Path('../models')\n",
    "RESULTS_DIR = Path('../results')\n",
    "\n",
    "# Création des répertoires\n",
    "for dir_path in [DATA_DIR, PROCESSED_DIR, MODELS_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données\n",
    "\n",
    "**Note**: Ce notebook suppose que les données IRM sont disponibles dans `data/raw/`. \n",
    "Pour utiliser des données de test, nous allons générer des données synthétiques ou utiliser un dataset public comme BraTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger les données IRM\n",
    "def load_mri_data(data_dir):\n",
    "    \"\"\"\n",
    "    Charge les données IRM depuis le répertoire spécifié.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (Path): Chemin vers le répertoire des données\n",
    "        \n",
    "    Returns:\n",
    "        images (list): Liste des chemins d'images\n",
    "        labels (list): Liste des labels (si disponibles)\n",
    "        metadata (DataFrame): Métadonnées des images\n",
    "    \"\"\"\n",
    "    # À implémenter selon la structure des données\n",
    "    # Pour l'instant, retourne des données factices\n",
    "    \n",
    "    # Exemple avec données synthétiques pour la démo\n",
    "    print(f\"Recherche des données dans: {data_dir}\")\n",
    "    \n",
    "    # Vérification de l'existence des données\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Le répertoire {data_dir} n'existe pas. Création...\")\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Création de données synthétiques pour la démo\n",
    "        print(\"Création de données synthétiques pour la démonstration...\")\n",
    "        # Cette partie serait remplacée par le chargement réel des données\n",
    "        \n",
    "    return [], [], pd.DataFrame()\n",
    "\n",
    "# Chargement des données\n",
    "images, labels, metadata = load_mri_data(DATA_DIR)\n",
    "print(f\"Nombre d'images chargées: {len(images)}\")\n",
    "print(f\"Nombre de labels: {len(labels) if labels else 'Aucun'}\")\n",
    "print(f\"Métadonnées: {metadata.shape if not metadata.empty else 'Aucune'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Préprocessing des images\n",
    "\n",
    "Prétraitement standard pour ResNet: redimensionnement, normalisation, augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations pour ResNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Taille d'entrée ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset personnalisé pour les IRM\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # À implémenter: charger l'image IRM\n",
    "        # Pour l'instant, retourne un tensor factice\n",
    "        image = torch.randn(3, 224, 224)  # Image factice\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        \n",
    "        return image\n",
    "\n",
    "print(\"Dataset et transformations définis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction de features avec ResNet\n",
    "\n",
    "Utilisation d'un ResNet pré-entraîné sur ImageNet comme extracteur de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle ResNet pré-entraîné\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()  # Mode évaluation\n",
    "\n",
    "# Modification pour extraire les features (supprimer la dernière couche)\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "feature_extractor.eval()\n",
    "\n",
    "print(f\"Modèle chargé sur: {device}\")\n",
    "print(f\"Nombre de paramètres: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse de dimensionnalité avec PCA et t-SNE\n",
    "\n",
    "Réduction de dimensionnalité pour visualiser la structure des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les features\n",
    "def extract_features(dataset, feature_extractor, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extrait les features d'un dataset d'images.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch, tuple):\n",
    "                images = batch[0]\n",
    "            else:\n",
    "                images = batch\n",
    "                \n",
    "            images = images.to(device)\n",
    "            batch_features = feature_extractor(images)\n",
    "            batch_features = batch_features.view(batch_features.size(0), -1)\n",
    "            features.append(batch_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(features)\n",
    "\n",
    "# Exemple d'extraction (à adapter avec des données réelles)\n",
    "print(\"Extraction de features... (exemple avec données factices)\")\n",
    "# features = extract_features(dataset, feature_extractor, device)\n",
    "# print(f\"Shape des features: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering pour labellisation faible\n",
    "\n",
    "Application de K-Means et DBSCAN pour générer des labels automatiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour déterminer le nombre optimal de clusters (méthode elbow)\n",
    "def find_optimal_clusters(features, max_k=10):\n",
    "    \"\"\"\n",
    "    Trouve le nombre optimal de clusters avec la méthode elbow.\n",
    "    \"\"\"\n",
    "    inertias = []\n",
    "    K = range(1, max_k + 1)\n",
    "    \n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(features)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    return inertias, K\n",
    "\n",
    "# Fonction pour évaluer la qualité des clusters\n",
    "def evaluate_clusters(features, labels):\n",
    "    \"\"\"\n",
    "    Évalue la qualité des clusters avec différentes métriques.\n",
    "    \"\"\"\n",
    "    silhouette = silhouette_score(features, labels)\n",
    "    davies_bouldin = davies_bouldin_score(features, labels)\n",
    "    \n",
    "    return {\n",
    "        'silhouette_score': silhouette,\n",
    "        'davies_bouldin_score': davies_bouldin\n",
    "    }\n",
    "\n",
    "print(\"Fonctions de clustering définies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des résultats\n",
    "\n",
    "Visualisation des clusters et analyse des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de visualisation\n",
    "def plot_clusters_2d(features_2d, cluster_labels, title=\"Clusters\"):\n",
    "    \"\"\"\n",
    "    Visualise les clusters en 2D.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                         c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_elbow_method(inertias, K):\n",
    "    \"\"\"\n",
    "    Trace la méthode elbow pour déterminer le k optimal.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K, inertias, 'bo-')\n",
    "    plt.xlabel('Nombre de clusters (k)')\n",
    "    plt.ylabel('Inertie')\n",
    "    plt.title('Méthode Elbow pour déterminer k optimal')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Fonctions de visualisation définies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Génération de labels faibles\n",
    "\n",
    "Création d'un dataset faiblement labellisé pour l'apprentissage semi-supervisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer les labels faibles\n",
    "def generate_weak_labels(features, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Génère des labels faibles via clustering.\n",
    "    \"\"\"\n",
    "    # Standardisation des features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    # Réduction de dimensionnalité avec PCA\n",
    "    pca = PCA(n_components=50)  # Réduction à 50 dimensions\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    # Clustering avec K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_pca)\n",
    "    \n",
    "    # Évaluation\n",
    "    metrics = evaluate_clusters(features_pca, cluster_labels)\n",
    "    \n",
    "    # Réduction à 2D pour visualisation\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features_pca)\n",
    "    \n",
    "    return {\n",
    "        'weak_labels': cluster_labels,\n",
    "        'features_2d': features_2d,\n",
    "        'metrics': metrics,\n",
    "        'pca': pca,\n",
    "        'kmeans': kmeans,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "print(\"Fonction de génération de labels faibles définie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pipeline complet\n",
    "\n",
    "Exécution du pipeline complet d'analyse exploratoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline complet\n",
    "def exploratory_pipeline(data_dir, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Pipeline complet d'analyse exploratoire.\n",
    "    \"\"\"\n",
    "    print(\"=== Début du pipeline d'analyse exploratoire ===\")\n",
    "    \n",
    "    # 1. Chargement des données\n",
    "    print(\"1. Chargement des données...\")\n",
    "    images, labels, metadata = load_mri_data(data_dir)\n",
    "    \n",
    "    # 2. Création du dataset\n",
    "    print(\"2. Création du dataset...\")\n",
    "    dataset = MRIDataset(images, labels, transform=transform)\n",
    "    \n",
    "    # 3. Extraction de features\n",
    "    print(\"3. Extraction de features avec ResNet...\")\n",
    "    features = extract_features(dataset, feature_extractor, device)\n",
    "    print(f\"   Features extraits: {features.shape}\")\n",
    "    \n",
    "    # 4. Génération de labels faibles\n",
    "    print(\"4. Génération de labels faibles via clustering...\")\n",
    "    results = generate_weak_labels(features, n_clusters=n_clusters)\n",
    "    \n",
    "    # 5. Visualisation\n",
    "    print(\"5. Visualisation des résultats...\")\n",
    "    plot_clusters_2d(results['features_2d'], results['weak_labels'], \n",
    "                    title=f\"Clusters (k={n_clusters})\")\n",
    "    \n",
    "    # 6. Métriques\n",
    "    print(\"6. Métriques d'évaluation:\")\n",
    "    for metric, value in results['metrics'].items():\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"=== Pipeline terminé ===\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Exécution du pipeline\n",
    "# results = exploratory_pipeline(DATA_DIR, n_clusters=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde des résultats\n",
    "\n",
    "Sauvegarde des features et labels faibles pour les étapes suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour sauvegarder les résultats\n",
    "def save_results(results, output_dir):\n",
    "    \"\"\"\n",
    "    Sauvegarde les résultats de l'analyse exploratoire.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sauvegarde des labels faibles\n",
    "    weak_labels_path = output_dir / \"weak_labels.npy\"\n",
    "    np.save(weak_labels_path, results['weak_labels'])\n",
    "    \n",
    "    # Sauvegarde des features\n",
    "    features_path = output_dir / \"features.npy\"\n",
    "    # np.save(features_path, features)  # À remplacer par les vraies features\n",
    "    \n",
    "    # Sauvegarde des métriques\n",
    "    metrics_path = output_dir / \"clustering_metrics.json\"\n",
    "    import json\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(results['metrics'], f, indent=2)\n",
    "    \n",
    "    # Sauvegarde des visualisations\n",
    "    plot_path = output_dir / \"clusters_plot.png\"\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"Résultats sauvegardés dans: {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'weak_labels': weak_labels_path,\n",
    "        'metrics': metrics_path,\n",
    "        'plot': plot_path\n",
    "    }\n",
    "\n",
    "# Sauvegarde exemple\n",
    "# save_results(results, RESULTS_DIR / \"exploratory_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook fournit le pipeline complet pour:\n",
    "1. L'analyse exploratoire des données IRM\n",
    "2. L'extraction de features avec ResNet\n",
    "3. La réduction de dimensionnalité avec PCA/t-SNE\n",
    "4. Le clustering pour la génération de labels faibles\n",
    "5. La visualisation et l'évaluation des résultats\n",
    "\n",
    "**Prochaines étapes**:\n",
    "1. Remplacer les données factices par des données IRM réelles\n",
    "2. Ajuster les hyperparamètres de clustering\n",
    "3. Tester différents algorithmes de clustering\n",
    "4. Valider les labels faibles avec un sous-ensemble de labels experts\n",
    "\n",
    "**Livrable**: Ce notebook constitue le Livrable 1 (Analyse exploratoire et clustering)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}